from typing import Dict, List, Optional, Type, cast
import os

from hydra._internal.utils import _locate
from hydra.utils import instantiate
from omegaconf import DictConfig, OmegaConf

from nuplan.planning.scenario_builder.abstract_scenario import AbstractScenario
from nuplan.planning.script.builders.model_builder import build_torch_module_wrapper
from nuplan.planning.script.builders.utils.utils_type import is_target_type
from nuplan.planning.simulation.planner.abstract_planner import AbstractPlanner
from nuplan.planning.simulation.planner.ml_planner.ml_planner import MLPlanner
from nuplan.planning.simulation.planner.ml_planner.diffusion_planner import DiffusionPlanner
from nuplan.planning.training.modeling.lightning_module_wrapper import LightningModuleWrapper
from nuplan.planning.simulation.planner.log_future_with_features_planner import LogFutureWithFeaturesPlanner


def _build_planner(planner_cfg: DictConfig, scenario: Optional[AbstractScenario]) -> AbstractPlanner:
    """
    Instantiate planner
    :param planner_cfg: config of a planner
    :param scenario: scenario
    :return AbstractPlanner
    """
    # Remove the thread_safe element given that it's used here
    config = planner_cfg.copy()
    if is_target_type(planner_cfg, MLPlanner):
        planner: AbstractPlanner = instantiate(config, scenario=scenario)
    elif is_target_type(planner_cfg, DiffusionPlanner):
        # Build model and feature builders needed to run an ML model in simulation
        config_path = '/'.join(config.checkpoint_path.split('/')[:-2])
        config_path = os.path.join(config_path, 'code/hydra/config.yaml')
        model_config = OmegaConf.load(config_path).model
        torch_module_wrapper = build_torch_module_wrapper(model_config)
        model = LightningModuleWrapper.load_from_checkpoint(
            planner_cfg.checkpoint_path, model=torch_module_wrapper
        ).model

        # Remove config elements that are redundant to MLPlanner
        OmegaConf.set_struct(config, False)
        config.pop('model_config')
        config.pop('checkpoint_path')
        OmegaConf.set_struct(config, True)

        planner: AbstractPlanner = instantiate(config, model=model, scenario=scenario)
    else:
        planner_cls: Type[AbstractPlanner] = _locate(config._target_)
        if planner_cls.requires_scenario:
            assert scenario is not None, (
                "Scenario was not provided to build the planner. " f"Planner {config} can not be build!"
            )
            planner = cast(AbstractPlanner, instantiate(config, scenario=scenario))
        else:
            planner = cast(AbstractPlanner, instantiate(config))

    return planner


def build_planners(
    planners_cfg: DictConfig, scenario: AbstractScenario, cache: Dict[str, AbstractPlanner] = dict()
) -> List[AbstractPlanner]:
    """
    Instantiate multiple planners by calling build_planner
    :param planners_cfg: planners config
    :param scenario: scenario
    :return planners: List of AbstractPlanners
    """
    planners = []
    for name in planners_cfg:
        config = planners_cfg[name].copy()
        thread_safe = config.thread_safe
        # Remove the thread_safe element given that it's used here
        OmegaConf.set_struct(config, False)
        config.pop('thread_safe')
        OmegaConf.set_struct(config, True)
        # Build the planner making sure to keep only 1 instance if it's non-thread-safe
        planner = cache.get(name, _build_planner(config, scenario))
        planners.append(planner)
        if not thread_safe and name not in cache:
            cache[name] = planner
    return planners
